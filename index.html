<!doctype html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="author" content="Man Luo">
  <!-- <meta property="og:image" content="images/drum.png" /> -->
  <title>Multimodal4Health ICHI 2024</title>
  <link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">
  <!-- <link rel="icon" type="image/png" href="images/drum.png"> -->
  <link href="https://use.fontawesome.com/releases/v5.0.4/css/all.css" rel="stylesheet">
  <meta name="mobile-web-app-capable" content="yes">
  <link rel="icon" href="./favicon.png" type="image/png" />

  <style>
    body {
      overflow-y: scroll;
    }
    .markdown-body h1 {
      display: flex;
    }
    .markdown-body form {
      margin-left: 10px;
    }
    .markdown-body input {
      margin: 0;
      padding: 0 10px;
      border: 1px solid #eaecef;
      border-radius: 3px;
      width: 100%;
      flex: 1 1;
    }
    table, tr, td {
      border: none;
    }
    table.fixed {table-layout:fixed; width:90px;}/*Setting the table width is important!*/
    table.fixed td {overflow:hidden;}/*Hide text outside the cell.*/
    table.fixed td:nth-of-type(1) {width:250px;}/*Setting the width of column 1.*/
    table.fixed td:nth-of-type(2) {width:250px;}/*Setting the width of column 2.*/
    table.fixed td:nth-of-type(3) {width:250px;}/*Setting the width of column 3.*/
    table.fixed td:nth-of-type(4) {width:250px;}/*Setting the width of column 4.*/
    table.fixed td:nth-of-type(5) {width:250px;}/*Setting the width of column 5.*/
  </style>
</head>

<body>
<div class="container" style="background-color: #fcfcfc;">
  <div class="row mb-3 justify-content-center">
    <div class="col-md-3" style="font-family: Lato, Helvetica, arial, sans-serif;">
      <!-- <img src="images/drum.png"  height=200px> -->
    </div>
    <!-- <div class="col-md-9"> -->
    <div class="container" style="background-color: #fcfcfc;">

      <br>
      <h1 >Multimodal4Health Workshop @ ICHI 2024</h1>
      <hr>
      <big>
        June 03, 2024 | 8:30 AM &ndash; 12:00 PM EST | Orlando, FL 
      </big>
      <br>
      <br>
      
        <h1>Description</h1>
        <p>
          The practice of modern medicine relies heavily on the synthesis of information and data from multiple sources; this includes imaging pixel data, structured laboratory data, unstructured narrative data, and in some cases, audio or observational data. 
          The integration of varied data modalities, ranging from the text in electronic health records and clinical notes to complex imaging like MRI, PET, histopathology images, and even video data from therapy consultations, and endoscopy, opens new avenues for groundbreaking patient care. For instance, in breast cancer management, combining patient history and physical examination data with mammography and genetic signatures can significantly enhance treatment planning. 
          In the context of healthcare domain, the application of multimodal learning extends beyond just understanding the status of individual organs to understanding the status of multi-organ systems with their complex interactions. This presents a more holistic view of patient health that mimics a physician's workflow.
        </p>
        <p>
          This workshop aims to tackle the unique and challenging fusion of diverse data types (e.g. text, high dimensional images, 1D signals) in healthcare. Unlike other domains, healthcare data demands measures to tackle extreme challenges related to data collection, missing data coding, ethical handling, particularly concerning patient privacy and data security, and the need for unparalleled accuracy in the downstream task with limited training data. 
        </p>
        <p>
          Recent successes in the generic computer science domain, such as large language models and stable diffusion image generation models, indicate the potential for transformative applications in healthcare. However, healthcare-specific challenges like handling high-resolution medical images without losing fine-grained details, addressing label imbalance and data sparsity, as well as managing long and time-sequenced data, present unique opportunities for innovation. 
Thus, the primary goal of this workshop is to bridge the `gap' between theoretical computer science advancements and practical healthcare applications, fostering interdisciplinary collaborations for holistic solutions. To achieve this goal, 
our workshop will feature leading researchers and experts from sub-disciplines including bioinformatics, natural language processing, computer vision, and multimodal representation learning. Through invited talks and panel discussions with industry and academia representatives, the workshop will explore the significant impact and challenges of multimodal learning in healthcare. By focusing on these critical aspects, the workshop is poised to set the stage for the next generation of healthcare solutions, driving forward the vision of AI in transforming healthcare through innovative multimodal learning approaches.
        </p>
    </div>
    <hr/><br/>
  </div>
</div>

<br>
<div class="container" style="background-color: #fcfcfc;">
  <h1>Schedule</h1>
  <!-- <div class="row col-md-12 justify-content-center"> -->
    <table>
      <tr>
        <td>08:30 &ndash; 08:40&nbsp;&nbsp;</td>
        <td>&nbsp;&nbsp; Welcome + Introduction to Multimodal4Health </td>
      </tr>
      <tr>
        <td>08:40 &ndash; 09:20&nbsp;&nbsp;</td>
        <td>&nbsp;&nbsp; Prof. Yifan Peng </td> 
        <td><i></b>Clinical natural language processing and deep learning in assisting medical image analysis</b></i></td>
      </tr>
      <tr><td>09:20 &ndash; 10:00&nbsp;&nbsp;</td>
      <td>&nbsp;&nbsp;Prof. Aaron Y. Lee</td>
      <td><i></b>Building a flagship, multimodal dataset for Type2 Diabetes</b></i></td>
      <tr><td>10:00 &ndash; 10:30&nbsp;&nbsp;</td><td>&nbsp;&nbsp;Poster Session + Coffee Break</td></tr>
      <tr>
        <td>10:30 &ndash; 11:10&nbsp;&nbsp;</td>
        <td>&nbsp;&nbsp;James Zou</td>
        <td><i><b>Generative AI for Healthcare</b></i></td>
      </tr>
      <tr>
        <td>11:10 &ndash; 11:50&nbsp;&nbsp;</td>
        <td>&nbsp;&nbsp;Selected Paper Presentation</td>
        <!-- <td><i><b>Multi-modality in 3D Scene Understanding</b></i></td> -->
      </tr>
      <tr><td>11:50 &ndash; 12:00&nbsp;&nbsp;</td><td>&nbsp;&nbsp;Close Remark</td></tr>
    </table>
  </div>
  <br/>
</div>

<div class="container" style="background-color: #fcfcfc;">
<h1>Call for Papers</h1>
The workshop will invite participation through paper submissions and poster presentations in the area of multimodal representation learning in healthcare. The topic of the workshop is very relevant to the current discourse of investigation of CV, NLP, and ML in healthcare being the immediate impacts on society and the biggest challenges faced by the community. We list prospective topics (but are not limited to) with specific applications in the healthcare domain:
<ul>
  <li> Domain generalization and adaptation for multimodal models; especially new findings / insights that agree or disagree with previous findings in vision-only tasks </li>
  <li> Adversarial robustness of multimodal systems </li>
  <li> Calibration and uncertainty estimation for multimodal models </li>
  <li> Detection and mitigation of fake and misleading images </li>
  <li> Watermarking, fingerprinting, or other preventative approaches for misuse of generative models </li>
  <li> Reliability from a human perspective, including investigations of bias, fairness, malicious use, users' perception of open-source multimodal systems</li>
  <li> Human-AI teaming for multimodal learning</li>
  <li> Ideas from human-computer interaction research that seek to improve reliability </li>
  <li> New benchmarks, datasets, evaluation metrics for testing open-domain reliability of multimodal systems </li>
  <li> Commentary and analysis of failure cases or negative results</li>
  <li> Out-of-Distribution detection for multimodal problems and for tasks beyond classification, detection, segmentation </li>
  <li> Fairness, explanability, interpretability of multimodal reasoning models </li>
  <li> Data-centric AI methods for cleaning, curating, and generating biomedical training data </li>
  
</ul>
<p>
  Our workshop invites 4-page papers that describe innovative ideas and developments. We will also accept ongoing work. The reviewing process will be double-blind. 
Authors will have an option to (1) opt into ICHI workshop proceedings or (2) a non-archival route that allows papers to be subsequently or concurrently submitted to other venues.
Submissions and reviews will not be public. Only accepted papers will be made public and publish in ICHI workshop proceedings.
</p>

<br/>
<h3>Important Dates:</h3>
  <table border="1" cellpadding="15">
    <tr>
      <td>&diams; <i>Submission Deadline:</i>&nbsp;&nbsp;</td><td>&nbsp;&nbsp;March 31st,  2024, Anywhere on Earth (UTC-12) </td>
    </tr>
    <tr>
      <td>&diams; <i>Notification of Decision:</i>&nbsp;&nbsp;</td><td>&nbsp;&nbsp;April 11, 2024, Anywhere on Earth (UTC-12) </td>
    </tr>
    <tr>
      <td>&diams; <i>Camera Ready Deadline:</i>&nbsp;&nbsp;</td><td>&nbsp;&nbsp;April 21st, 2024, Anywhere on Earth (UTC-12) </td>
    </tr>
    <!-- <tr>
      <td>&diams; <i>Submission website (CMT):</i></td>
      <td> <a href="https://cmt3.research.microsoft.com/ODRUM2023/">https://cmt3.research.microsoft.com/ODRUM2023/</a> </td>
    </tr> -->
  </table>
<br/>
<h3>Submission Link:</h3>
<table border="1" cellpadding="15">
  <tr>
    <td>&diams; Submit your manuscript through <a href="https://easychair.org/conferences/?conf=ieeeichi2024">EasyChair</a>, and select Multimodal4Health Workshop.</td>
  </tr>
</table>
<br/>

</div>
<br/>
<div class="container" style="background-color: #fcfcfc;">
  <h2>Invited Speakers</h2>
  <div class="row col-md-12 justify-content-center">
    <div class="profile">
      <a href="https://scholar.google.com/citations?user=E84LrtYAAAAJ&hl=en">
        <img class="profile_img" height="160px" style="border-radius:30%" src="images/lee.png" >
        <br><big><b>Aaron Y. Lee</b></big>
        <br/>Associate Professor
        <br/>UW Medical Center
      </a>
      <!-- <br/>(Invited Talk: 1600 &ndash; 1700 PDT) -->
    </div>

    <div class="profile">
      <a href="https://profiles.stanford.edu/james-zou">
        <img class="profile_img" height="160px" style="border-radius:30%" src="images/james_zhou.png" >
        <br><big><b>James Zou</b></big>
        <br/>Professor
        <br/>Stanford University
      </a>
      <!-- <br/>(Panel: 1600 &ndash; 1700 PDT) -->
    </div>

    <div class="profile">
      <a href="https://penglab.weill.cornell.edu/team/yifan-peng">
        <img class="profile_img" height="160px" style="border-radius:30%" src="images/peng.png" >
        <br><big><b>Yifan Peng</b></big>
        <br/>Assistant Professor
        <br/>Weill Cornell Medicine
      </a>
      <!-- <br/>(Panel: 1600 &ndash; 1700 PDT) -->
    </div>
  </div>
</div>




<!-- <br> -->
<!-- <div class="container" style="background-color: #fcfcfc;"> -->
  <!-- <h2>Accepted Papers</h2> -->
  <!-- <center><i><big>Link to <a href="https://openaccess.thecvf.com/CVPR2023_workshops/O-DRUM">O-DRUM 2023 Proceedings (on CVF Open Access)</a></big></i></center> -->
  <!-- <br> -->
  <!-- <div class="row col-md-12 justify-content-center">
    <details><summary><big>Click here to display List of all accepted papers</big></summary>
      <ul>
      <li>
        Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering
        <br/> <i>Pan Lu (The University of California, Los Angeles)*; Swaroop Ranjan Mishra (self); Tony Xia (UCLA); Liang Qiu (University of California, Los Angeles); Kai-Wei Chang (UCLA); Song-Chun Zhu (ucla); Oyvind Tafjord (AI2); Peter Clark (Allen Institute for AI); Ashwin Kalyan (Georgia Institute of Technology)</i>
      </li>
      
    </ul>
    </details>

  </div> -->
<!-- </div> -->

<!-- 
<br/> -->
<br>
<div class="container" style="background-color: #fcfcfc;">
  <h2>Organizers</h2>
  <div class="row col-md-12 justify-content-center">
    <div class="profile">
      <a href="https://luomancs.github.io/">
        <!-- <img class="profile_img" height="160px" style="border-radius:30%" src="images/man.jpg"> -->
      <br><b>Man Luo</b></a>
      <br/>Postdoctoral Researcher
      <br/>Mayo Clinic
    </div>
    <div class="profile">
      <a href="https://aishaurooj.wixsite.com/aishaurooj">
        <!-- <img class="profile_img" height="160px" style="border-radius:30%" src="images/yezhou.png"> -->
      <br><b>Aisha Urooj </b></a>
      <br/>Postdoctoral Researcher
      <br/>Mayo Clinic
    </div> 
    <div class="profile">
      <a href="https://scholar.google.co.id/citations?user=bgn8HTQAAAAJ&hl=en">
        <!-- <img class="profile_img" height="160px" style="border-radius:30%" src="images/cbaral.png"> -->
        <br><b>Theo Dapamede</b></a>
        <br>Postdoctoral
        <br>Emory University<br>
    </div>
    <div class="profile">
      <a href="https://web.stanford.edu/~jfries/">
        <!-- <img class="profile_img" height="160px" style="border-radius:30%" src="images/kenny.PNG"> -->
      <br><b>Jason Fries</b></a>
      <br>Research Scientist
      <br>Standford University
    </div>
    <div class="profile">
      <a href="https://www.mayo.edu/research/faculty/patel-bhavik-n-m-d-m-b-a/bio-20518426">
        <!-- <img class="profile_img" height="160px" style="border-radius:30%" src="images/jacob.jpg"> -->
      <br><b>Bhavik Patel</b></a>
      <br>AI Lead
      <br>Mayo Clinic
    </div>
    <div class="profile">
      <a href="https://labs.engineering.asu.edu/banerjeelab/person/imon-banerjee/">
        <!-- <img class="profile_img" height="160px" style="border-radius:30%" src="images/jacob.jpg"> -->
      <br><b>Imon Banerjee</b></a>
      <br>AI Lead
      <br>Mayo Clinic
    </div>
    
  </div>

  <!-- <br> -->
<div class="container" style="background-color: #fcfcfc;">
  <h2>Committee</h2>
  <ul>
    <li>Chen Chen (University of Central Florida, FL)</li>
    <li>Guangyu Sun (University of Central Florida, FL)</li>
    <li>Ulas Bagci (Northwestern University, IL)</li>
    <li>Amara Tariq (Mayo Clinic, AZ)</li>
    <li>Avisha Das (Mayo Clinic, AZ)</li>
    <li>Jiwoong Jeong (Arizona State University, Mayo Clinic, AZ)</li>
    <li>Ramon Correa (Arizona State University, Mayo Clinic, AZ)</li>
    <li>Mihir Parmar (Arizona State University, AZ)</li>
    <li>Jay Shah (Arizona State University, AZ) </li>
  </ul>
  
    
</div>



      <hr>
      Please contact Man Luo (<a href="mailto: mluo26@asu.edu">mluo26@asu.edu</a>) for additional details
      <!-- <hr> -->
      <!-- <i> The workshop is supported by US National Science Foundation grants 1816039, 2132724 as part of Research, Education, and Outreach activities. </i> -->
      <!-- <hr> -->
      <p align='right'><i>Website maintained by Man Luo </i> </p>

  </div>
<!-- </div> -->
</body>
</html>
